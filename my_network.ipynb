{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "856ca84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using PaddedViews\n",
    "using LinearAlgebra\n",
    "using TimerOutputs\n",
    "using BenchmarkTools\n",
    "using Profile\n",
    "to = TimerOutput();\n",
    "tt = TimerOutput();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ed982467",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type Node end\n",
    "abstract type Operator <: Node end\n",
    "\n",
    "mutable struct Variable{N} <: Node\n",
    "    name::String\n",
    "    output::Array{Float64, N}\n",
    "    gradient::Array{Float64, N}\n",
    "    v₁::Array{Float64, N}\n",
    "    v₂::Array{Float64, N}\n",
    "    Variable(N, output; name = \"?\") = new{N}(name, output, zeros(size(output)), zeros(size(output)), zeros(size(output)))\n",
    "end\n",
    "\n",
    "mutable struct NodeOperator{F, N} <: Operator\n",
    "    name::String\n",
    "    inputs::Vector{Node}\n",
    "    output::Array{Float64, N}\n",
    "    gradient::Array{Float64, N}\n",
    "    NodeOperator(fun, inputs...; name = \"?\", shape=(1,1,1)) = new{typeof(fun), length(shape)}(name, [inputs...], zeros(shape), zeros(shape))\n",
    "end\n",
    "\n",
    "mutable struct DenseOperator{F, N} <: Operator\n",
    "    name::String\n",
    "    inputs::Vector{Node}\n",
    "    output::Array{Float64, N}\n",
    "    gradient::Array{Float64, N}\n",
    "    DenseOperator(fun, inputs...; name = \"?\", shape=(1,1,1)) = new{typeof(fun), length(shape)}(name, [inputs...], zeros(shape), zeros(shape))\n",
    "end\n",
    "\n",
    "mutable struct CrossEntropyOperator{F, N} <: Operator\n",
    "    name::String\n",
    "    inputs::Vector{Node}\n",
    "    output::Array{Float64, N}\n",
    "    gradient::Array{Float64, N}\n",
    "    CrossEntropyOperator(fun, inputs...; name = \"?\", shape=(1,1,1)) = new{typeof(fun), length(shape)}(name, [inputs...], zeros(shape), zeros(shape))\n",
    "end\n",
    "\n",
    "mutable struct SoftMaxOperator{F, N} <: Operator\n",
    "    name::String\n",
    "    inputs::Vector{Node}\n",
    "    output::Array{Float64, N}\n",
    "    gradient::Array{Float64, N}\n",
    "    J::Array{Float64, 2}\n",
    "    SoftMaxOperator(fun, inputs...; name = \"?\", shape=(1,1,1)) = new{typeof(fun), length(shape)}(name, [inputs...], zeros(shape), zeros(shape),zeros(shape,shape))\n",
    "end\n",
    "\n",
    "mutable struct RNNOperator{F, N} <: Operator\n",
    "    name::String\n",
    "    h::Array{Float64,2}\n",
    "    inputs::Vector{Node}\n",
    "    output::Array{Float64, N}\n",
    "    gradient::Array{Float64, N}\n",
    "    gradienth::Array{Float64, 2}\n",
    "    gradientW::Array{Float64, 2}\n",
    "    gradientU::Array{Float64, 2}\n",
    "    gradientB::Array{Float64, 1}\n",
    "    RNNOperator(fun, h, inputs...; name = \"?\", shape=(1,1,1)) = new{typeof(fun), length(shape)}(name,h, [inputs...], zeros(shape), zeros(shape),zeros(size(h)[1],layerNumber),zeros(size(inputs[2].output)),zeros(size(inputs[3].output)),zeros(size(inputs[4].output)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "da09b989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_graph (generic function with 1 method)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function visit(node::Node, visited::Set, order::Vector)\n",
    "    if node ∉ visited\n",
    "        push!(visited, node)\n",
    "        push!(order, node)\n",
    "    end\n",
    "end\n",
    "\n",
    "function visit(node::Operator, visited::Set, order::Vector)\n",
    "    if node ∉ visited\n",
    "        for input in node.inputs\n",
    "            visit(input, visited, order)\n",
    "        end\n",
    "        push!(visited, node)\n",
    "        push!(order, node)\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function create_graph(root::Node)\n",
    "    visited = Set{Node}()\n",
    "    order = Vector{Node}()\n",
    "    visit(root, visited, order)\n",
    "    return order\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d4ebd648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show (generic function with 605 methods)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base: show, summary\n",
    "show(io::IO, x::NodeOperator{F}) where {F} = print(io, \"op \", \"(\", F, \")\");\n",
    "show(io::IO, x::RNNOperator{F}) where {F} = print(io, \"op \", \"(\", F, \")\");\n",
    "show(io::IO, x::SoftMaxOperator{F}) where {F} = print(io, \"op \", \"(\", F, \")\");\n",
    "show(io::IO, x::CrossEntropyOperator{F}) where {F} = print(io, \"op \", \"(\", F, \")\");\n",
    "show(io::IO, x::DenseOperator{F}) where {F} = print(io, \"op \", \"(\", F, \")\");\n",
    "show(io::IO, x::Variable) = begin\n",
    "    print(io, \"var \", x.name);\n",
    "    print(io, \"\\n ┣━ ^ \"); summary(io, x.output)\n",
    "    print(io, \"\\n ┗━ ∇ \");  summary(io, x.gradient)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "783ce604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zero_gradient! (generic function with 2 methods)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_gradient!(node::Node) = fill!(node.gradient, 0)\n",
    "function zero_gradient!(order::Vector{Node})\n",
    "    for node in order\n",
    "        zero_gradient!(node)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "98249c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct RNNParams\n",
    "    W::Variable{2}\n",
    "    U::Variable{2}\n",
    "    b::Variable{1}\n",
    "end\n",
    "\n",
    "struct DenseParams\n",
    "    weights::Variable{2}\n",
    "    bias::Variable{1}\n",
    "end\n",
    "struct NetworkParams\n",
    "    rnn::RNNParams\n",
    "    dense::DenseParams\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "38707839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 4 methods)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recurent_layer(x::Node, h, W::Node,U::Node, b::Node, shape) = RNNOperator(recurent_layer,h,name=\"rnn\",shape=shape,x,W,U,b)\n",
    "@inbounds @views @timeit to \"recurent forward\" forward(node::RNNOperator{typeof(recurent_layer)}, x, W, U, b) = let\n",
    "   fill!(node.h[:,1],0)\n",
    "   mul!(node.gradientB,W, node.h[:,1])\n",
    "   mul!(node.gradient, U,  x[1:recsize])\n",
    "      node.h[:,1] .= tanh.(node.gradientB.+node.gradient.+b)   \n",
    "       for t in range(2,layerNumber)\n",
    "        mul!(node.gradientB,W, node.h[:,t-1])\n",
    "       mul!(node.gradient, U,  x[(t.-1).*(recsize).+1:(t).*(recsize)])\n",
    "       node.h[:,t] .= tanh.(node.gradientB.+node.gradient.+b)   \n",
    "    end\n",
    "    return node.h[:,layerNumber]\n",
    "end\n",
    "\n",
    "@inbounds @views @timeit to \"recurent backward\" backward(node::RNNOperator{typeof(recurent_layer)}, x::Array{Float64,1}, W::Array{Float64,2}, U::Array{Float64,2}, b::Array{Float64,1}, g::Array{Float64,1}) = let\n",
    "   \n",
    "   node.gradienth[:,layerNumber] .= node.gradient\n",
    "   t = layerNumber-1\n",
    "    while t >=1\n",
    "     mul!(node.gradienth[:,t],W', node.gradienth[:,t.+1])\n",
    "     node.gradienth[:,t] .= node.gradienth[:,t] .* (1 .-  node.h[:,t+1].^2)\n",
    "    t-=1\n",
    " end\n",
    " t = layerNumber\n",
    "  while t>=1\n",
    "    @views xc = x[(t-1).*(recsize).+1:(t).*(recsize)]\n",
    "    node.gradientB .=  (1 .- node.h[:,t].^2) .* node.gradienth[:,t];\n",
    "     mul!(node.gradientU,node.gradientB , xc')\n",
    "      node.inputs[3].gradient .+=  node.gradientU\n",
    "    if t !=1\n",
    "        mul!(node.gradientW , node.gradientB , node.h[:,t-1]')\n",
    "        node.inputs[2].gradient .+= node.gradientW\n",
    "    end\n",
    "    t-=1\n",
    " end\n",
    "    node.inputs[4].gradient .= node.gradientB\n",
    "end \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ba0a1cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 4 methods)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_layer(x::Node, w::Node, b::Node, shape) = DenseOperator(dense_layer, name=\"dense\", shape=shape, x, w, b)\n",
    "\n",
    "@views @timeit to \"dense forward\" forward(node::DenseOperator{typeof(dense_layer)}, x, w, b) = let\n",
    "    mul!(node.output,w,x)\n",
    "    node.output .+= b\n",
    "    return node.output\n",
    "end\n",
    "\n",
    "@inbounds @views @timeit to \"dense backward\" backward(node::DenseOperator{typeof(dense_layer)}, x, w, b, g) = let\n",
    "    mul!(node.inputs[1].gradient , w' , g)\n",
    "    mul!(node.inputs[2].gradient , g , x')\n",
    "    node.inputs[3].gradient .= g\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "349a7391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 4 methods)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x::Node, shape) = SoftMaxOperator(softmax, name=\"softmax\", shape=shape, x)\n",
    "\n",
    "@views @timeit to \"softmax forward\" forward(node::SoftMaxOperator{typeof(softmax)}, x::Vector{Float64}) = let\n",
    "     node.gradient .= exp.(x)\n",
    "     return node.gradient ./ sum(node.gradient::Vector{Float64})\n",
    "end\n",
    "\n",
    "@inbounds @views @timeit to \"softmax backward\" backward(node::SoftMaxOperator{typeof(softmax)}, x::Vector{Float64}, g) = let\n",
    "   y = node.output\n",
    "   mul!(node.J, .-y, y')\n",
    "   node.J[1:11:100] .+= y #prev diagind(node.J)\n",
    "   mul!(node.inputs[1].gradient , node.J , g)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "48f85779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 4 methods)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(ŷ::Node, y::Node, shape) = CrossEntropyOperator(cross_entropy_loss, name=\"cross_entropy_loss\", shape=shape,  ŷ, y)\n",
    "\n",
    "@timeit to \"cross_entropy_loss forward\" forward(::CrossEntropyOperator{typeof(cross_entropy_loss)}, ŷ, y) = let\n",
    "    return sum((ŷ.-y) .^ 2 ./ 10)\n",
    "end\n",
    "\n",
    "@timeit to \"cross_entropy_loss backward\" backward(node::CrossEntropyOperator{typeof(cross_entropy_loss)}, ŷ, y, g) = let\n",
    "    node.inputs[1].gradient .= (ŷ.-y)./5\n",
    "    node.inputs[2].gradient .=[0.0]\n",
    "end\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a4db448f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 1 method)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zero_gradient!(node::Node) = fill!(node.gradient, 0)\n",
    "\n",
    "compute!(node::Variable) = nothing\n",
    "#compute!(node::Operator) = node.output .= forward(node, [input.output for input in node.inputs]...)\n",
    "@views compute!(node::RNNOperator{typeof(recurent_layer)}) =  node.output .= forward(node, node.inputs[1].output,node.inputs[2].output,node.inputs[3].output,node.inputs[4].output)\n",
    "@views compute!(node::SoftMaxOperator{typeof(softmax)}) = node.output .= forward(node, node.inputs[1].output)\n",
    "@views compute!(node::DenseOperator{typeof(dense_layer)}) =  node.output .= forward(node, node.inputs[1].output,node.inputs[2].output,node.inputs[3].output)\n",
    "@views compute!(node::CrossEntropyOperator{typeof(cross_entropy_loss)}) =  node.output .= forward(node, node.inputs[1].output,node.inputs[2].output)\n",
    "\n",
    "@inbounds function forward!(order::Vector{Node})::Float64\n",
    "    for node in order\n",
    "        compute!(node)\n",
    "    end\n",
    "    \n",
    "    return last(order).output[1]\n",
    "end   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a28d4e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 6 methods)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update!(node::Node, gradient) =  node.gradient .+= gradient\n",
    "\n",
    "\n",
    "function backward!(order::Vector{Node})\n",
    "   last(order).gradient = [1.0]\n",
    "    \n",
    "     @inbounds for node in Iterators.reverse(order)\n",
    "        backward!(node)\n",
    "    end\n",
    "end\n",
    "\n",
    "backward!(node::Variable) = nothing\n",
    "\n",
    "function backward!(node::RNNOperator{typeof(recurent_layer)})\n",
    "     backward(node, node.inputs[1].output,node.inputs[2].output,node.inputs[3].output,node.inputs[4].output, node.gradient)\n",
    "end\n",
    "function backward!(node::SoftMaxOperator{typeof(softmax)})\n",
    "     backward(node, node.inputs[1].output, node.gradient)\n",
    "end\n",
    "function backward!(node::DenseOperator{typeof(dense_layer)})\n",
    "     backward(node, node.inputs[1].output,node.inputs[2].output,node.inputs[3].output, node.gradient)\n",
    "end\n",
    "function backward!(node::CrossEntropyOperator{typeof(cross_entropy_loss)})\n",
    "     backward(node, node.inputs[1].output,node.inputs[2].output, node.gradient)\n",
    "end\n",
    "#function backward!(node::Operator)\n",
    "#    backward(node, [input.output for input in node.inputs]..., node.gradient)\n",
    "#end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "551e0a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_network (generic function with 1 method)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function create_network(x::Variable{1}, y::Variable{1}, params::NetworkParams)\n",
    "    x₁ = recurent_layer(x,zeros(64,layerNumber),params.rnn.W,params.rnn.U,params.rnn.b,(64))\n",
    "    ŷ  = dense_layer(x₁, params.dense.weights, params.dense.bias,(10))\n",
    "    loss = cross_entropy_loss(ŷ, y, (1))\n",
    "    return create_graph(loss)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "41d18687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "he_weights_init (generic function with 1 method)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function he_weights_init(prev, shape...)\n",
    "    std = sqrt(2.0./prev)\n",
    "    weights = rand(Float64, shape) .*2 .-1\n",
    "    return weights .* std\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7565f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mutable struct Adam\n",
    "    α::Float64\n",
    "    ε::Float64\n",
    "    m₁::Float64\n",
    "    m₂::Float64\n",
    "    k::Int64\n",
    "    \n",
    "    Adam(α=0.001, m₁=0.9, m₂=0.999, ε=1e-8) = new(α, ε, m₁, m₂, 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a6961444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_weights_N! (generic function with 1 method)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update_weights!(graph::Vector{Node}, M::Adam)\n",
    "     for node in graph\n",
    "        if (typeof(node) == Variable{1} || typeof(node) == Variable{2}) && (node.name !=\"x\"&&node.name!=\"y\")\n",
    "            update_weights_N!(node, M)\n",
    "        end\n",
    "    end\n",
    "    M.k += 1\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function update_weights_N!(node::Variable{T}, M::Adam) where T\n",
    "    g =  node.gradient\n",
    "    v₁ =  node.v₁\n",
    "    v₂ =  node.v₂\n",
    "    m₁, m₂, k, α, ε = M.m₁, M.m₂, M.k, M.α, M.ε\n",
    "    v₁ .= @. m₁ * v₁ .+ (1.0 .- m₁) .* g\n",
    "    v₂ .= @.  m₂ * v₂ .+ (1.0 .-  m₂) .* (g .* g)\n",
    "     mt₁ = m₁.^k\n",
    "    mt₂ = m₂.^k\n",
    "    α₁ = α .* sqrt.(1 - mt₂)/(1-mt₁)\n",
    "    node.output .-= @. α₁*v₁ ./ (sqrt.(v₂) + ε)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ec8ec226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "validate (generic function with 1 method)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@inbounds @views function validate(x, y, graph, x_data,y_data)::Float64\n",
    "    correct = 0\n",
    "    length = size(y_data)[2]\n",
    "    for i in range(1,length)\n",
    "        x.output .= x_data[:,i]\n",
    "        y.output .= y_data[:,i]\n",
    "        forward!(graph)\n",
    "        pred = argmax(graph[8].output)\n",
    "        if 1 == y_data[:,i][pred]\n",
    "               correct += 1\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    acc_val = correct./size(y_data)[2]\n",
    "    \n",
    "    return acc_val\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ade2daf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetworkParams(RNNParams(var W1\n",
       " ┣━ ^ 64×64 Matrix{Float64}\n",
       " ┗━ ∇ 64×64 Matrix{Float64}, var U1\n",
       " ┣━ ^ 64×196 Matrix{Float64}\n",
       " ┗━ ∇ 64×196 Matrix{Float64}, var b1\n",
       " ┣━ ^ 64-element Vector{Float64}\n",
       " ┗━ ∇ 64-element Vector{Float64}), DenseParams(var w2\n",
       " ┣━ ^ 10×64 Matrix{Float64}\n",
       " ┗━ ∇ 10×64 Matrix{Float64}, var b2\n",
       " ┣━ ^ 10-element Vector{Float64}\n",
       " ┗━ ∇ 10-element Vector{Float64}))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const recsize = 14*14\n",
    "\n",
    "recurent = RNNParams(\n",
    "    Variable(2, he_weights_init(64, 64, 64), name=\"W1\"),\n",
    "    Variable(2, he_weights_init(64, 64, recsize), name=\"U1\"),\n",
    "    Variable(1, zeros(64), name=\"b1\")\n",
    ")\n",
    "dense = DenseParams(\n",
    "    Variable(2, he_weights_init(64, 10, 64), name=\"w2\"),\n",
    "    Variable(1, zeros(10), name=\"b2\")\n",
    ")\n",
    "networkparams = NetworkParams(recurent,dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0798c4f4-b616-4f8d-aebb-eeb66b89507c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Node}:\n",
       " var x\n",
       " ┣━ ^ 784-element Vector{Float64}\n",
       " ┗━ ∇ 784-element Vector{Float64}\n",
       " var W1\n",
       " ┣━ ^ 64×64 Matrix{Float64}\n",
       " ┗━ ∇ 64×64 Matrix{Float64}\n",
       " var U1\n",
       " ┣━ ^ 64×196 Matrix{Float64}\n",
       " ┗━ ∇ 64×196 Matrix{Float64}\n",
       " var b1\n",
       " ┣━ ^ 64-element Vector{Float64}\n",
       " ┗━ ∇ 64-element Vector{Float64}\n",
       " op (typeof(recurent_layer))\n",
       " var w2\n",
       " ┣━ ^ 10×64 Matrix{Float64}\n",
       " ┗━ ∇ 10×64 Matrix{Float64}\n",
       " var b2\n",
       " ┣━ ^ 10-element Vector{Float64}\n",
       " ┗━ ∇ 10-element Vector{Float64}\n",
       " op (typeof(dense_layer))\n",
       " var y\n",
       " ┣━ ^ 10-element Vector{Float64}\n",
       " ┗━ ∇ 10-element Vector{Float64}\n",
       " op (typeof(cross_entropy_loss))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opracowane na podstawie https://minpy.readthedocs.io/en/latest/tutorial/rnn_mnist.html\n",
    "using MLDatasets, Flux\n",
    "train_data = MLDatasets.MNIST(split=:train)\n",
    "test_data  = MLDatasets.MNIST(split=:test)\n",
    "\n",
    "function loader(data)\n",
    "    x1dim = reshape(data.features, 28 * 28, :) # reshape 28×28 pixels into a vector of pixels\n",
    "    yhot  = Flux.onehotbatch(data.targets, 0:9) # make a 10×60000 OneHotMatrix\n",
    "    (x1dim, yhot)\n",
    "end\n",
    "(x_data,y_data) = loader(train_data)\n",
    "(x_test,y_test) = loader(test_data)\n",
    "x::Variable{1} = Variable(1, x_data[:,1], name=\"x\")\n",
    "y::Variable{1} = Variable(1, y_data[:,1], name=\"y\")\n",
    "const layerNumber = floor(Int,size(x_data[:,1])[1]/recsize)\n",
    "net = create_network(x,y,networkparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4372550b-a7e1-4f2b-88b0-7dac8e5669be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_rnn (generic function with 1 method)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_rnn(x_data,y_data,net,x,y)\n",
    "loss::Float64 = 0.0\n",
    "pred::UInt8 = 0\n",
    "epochs = 5\n",
    "batch_size = 100\n",
    "losses = zeros(epochs)\n",
    "acc = zeros(epochs)\n",
    "test_acc = zeros(epochs)\n",
    "correct = 0 \n",
    "adam = Adam()\n",
    "for epoch in 1:epochs\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    length = size(y_data)[2]\n",
    "    @time for i in range(1,length)\n",
    "         x.output .= @view x_data[:,i]\n",
    "         y.output .= @view y_data[:,i]\n",
    "        @timeit to \"forward\" loss += forward!(net)\n",
    "        prob::DenseOperator{typeof(dense_layer)} = net[8]\n",
    "        pred = argmax(prob.output)\n",
    "        if 1 == y_data[:,i][pred]\n",
    "            correct += 1\n",
    "        end\n",
    "        @timeit to \"backward\" backward!(net)\n",
    "        if i % batch_size == 0\n",
    "        @timeit to \"update weights\" update_weights!(net, adam)\n",
    "        zero_gradient!(net)\n",
    "        end;\n",
    "    end\n",
    "    losses[epoch] = loss/length\n",
    "    acc[epoch] = correct/length\n",
    "    @timeit to \"validate\" test_acc[epoch] = validate(x,y,net,x_test,y_test)\n",
    "    println(\"Epoch: \", epoch, \"\\tAverage loss: \", round(losses[epoch], digits=3), \"\\tAverage acc: \", round(acc[epoch],digits=3),\"\\tAverage test acc: \",round(test_acc[epoch],digits=3))\n",
    "end\n",
    "show(to)\n",
    "#show(tt)\n",
    "reset_timer!(to);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a3c43f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.801079 seconds (4.16 M allocations: 155.722 MiB, 0.22% gc time, 15.88% compilation time)\n",
      "Epoch: 1\tAverage loss: 0.045\tAverage acc: 0.813\tAverage test acc: 0.889\n",
      "  5.432552 seconds (2.82 M allocations: 64.114 MiB)\n",
      "Epoch: 2\tAverage loss: 0.028\tAverage acc: 0.901\tAverage test acc: 0.912\n",
      "  5.472692 seconds (2.82 M allocations: 64.114 MiB)\n",
      "Epoch: 3\tAverage loss: 0.024\tAverage acc: 0.918\tAverage test acc: 0.918\n",
      "  5.284816 seconds (2.82 M allocations: 64.114 MiB, 0.54% gc time)\n",
      "Epoch: 4\tAverage loss: 0.023\tAverage acc: 0.922\tAverage test acc: 0.92\n",
      "  5.268782 seconds (2.82 M allocations: 64.114 MiB)\n",
      "Epoch: 5\tAverage loss: 0.021\tAverage acc: 0.927\tAverage test acc: 0.926\n",
      "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────────────────\u001b[22m\n",
      "\u001b[0m\u001b[1m                               \u001b[22m         Time                    Allocations      \n",
      "                               ───────────────────────   ────────────────────────\n",
      "       Tot / % measured:            31.5s /  90.6%            739MiB /  58.6%    \n",
      "\n",
      " Section               ncalls     time    %tot     avg     alloc    %tot      avg\n",
      " ────────────────────────────────────────────────────────────────────────────────\n",
      " backward                300k    21.1s   74.0%  70.2μs    220MiB   50.9%     771B\n",
      "   recurent backward     300k    19.2s   67.5%  64.1μs   77.8MiB   18.0%     272B\n",
      "   dense backward        300k    520ms    1.8%  1.73μs   27.5MiB    6.3%    96.0B\n",
      "   cross_entropy_lo...   300k    143ms    0.5%   477ns   41.2MiB    9.5%     144B\n",
      " forward                 300k    5.80s   20.3%  19.3μs    157MiB   36.3%     550B\n",
      "   recurent forward      300k    4.12s   14.5%  13.7μs   9.16MiB    2.1%    32.0B\n",
      "   dense forward         300k    150ms    0.5%   500ns   9.16MiB    2.1%    32.0B\n",
      "   cross_entropy_lo...   300k    143ms    0.5%   476ns   54.9MiB   12.7%     192B\n",
      " validate                   5    995ms    3.5%   199ms   31.3MiB    7.2%  6.25MiB\n",
      "   recurent forward     50.0k    646ms    2.3%  12.9μs   1.53MiB    0.4%    32.0B\n",
      "   dense forward        50.0k   21.4ms    0.1%   428ns   1.53MiB    0.4%    32.0B\n",
      "   cross_entropy_lo...  50.0k   15.4ms    0.1%   308ns   9.16MiB    2.1%     192B\n",
      " update weights         3.00k    626ms    2.2%   209μs   24.0MiB    5.5%  8.20KiB\n",
      "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────────────────\u001b[22m\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────\u001b[22m\n",
      "\u001b[0m\u001b[1m                   \u001b[22m         Time                    Allocations      \n",
      "                   ───────────────────────   ────────────────────────\n",
      " Tot / % measured:      31.6s /   0.0%            739MiB /   0.0%    \n",
      "\n",
      " Section   ncalls     time    %tot     avg     alloc    %tot      avg\n",
      " ────────────────────────────────────────────────────────────────────\n",
      "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────\u001b[22m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────\u001b[22m\n",
       "\u001b[0m\u001b[1m                   \u001b[22m         Time                    Allocations      \n",
       "                   ───────────────────────   ────────────────────────\n",
       " Tot / % measured:      343ms /   0.0%           20.3MiB /   0.0%    \n",
       "\n",
       " Section   ncalls     time    %tot     avg     alloc    %tot      avg\n",
       " ────────────────────────────────────────────────────────────────────\n",
       "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────\u001b[22m"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " train_rnn(x_data,y_data,net,x,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
