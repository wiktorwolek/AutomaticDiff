# AutomaticDiff

AutomaticDiff is a Julia-based project that implements **automatic differentiation (AD) from scratch** for simple Recurrent Neural Networks (RNNs).
It is designed as an educational tool to help understand how AD works in neural networks without relying on external deep learning libraries.

## Features

* Manual implementation of automatic differentiation
* Focused on simple RNN architectures
* Educational purpose for learning AD concepts

## Installation

This project uses Jupyter notebooks with Julia. To run it locally:

1. Clone the repository:

```bash
git clone https://github.com/wiktorwolek/AutomaticDiff.git
cd AutomaticDiff
```

2. Install required Julia packages

3. Launch Jupyter with Julia

4. Open the notebook files from the repository.

> ⚠️ The notebooks are self-contained and do not require any external dependencies beyond standard Julia packages.

## Usage

Open any of the Jupyter notebooks in the repository to explore the implementation:

* `AWID-2024-RNN.ipynb` — main RNN implementation with automatic differentiation
* `my_network.ipynb` — earlier version of the network
* `my_network-version-1.ipynb` — another variant

Each notebook walks you through the AD implementation step by step.

## Contributing

This is a personal project, but anyone is free to **fork the repository** and continue development. Pull requests and suggestions are welcome.

## License

This project is licensed under the **MIT License** — you are free to use, modify, and distribute it.
See the [LICENSE](LICENSE) file for details.
