{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "856ca84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using PaddedViews\n",
    "using LinearAlgebra\n",
    "using TimerOutputs\n",
    "using BenchmarkTools\n",
    "using Profile\n",
    "to = TimerOutput();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed982467",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type Node end\n",
    "abstract type Operator <: Node end\n",
    "\n",
    "mutable struct Variable{N} <: Node\n",
    "    name::String\n",
    "    output::Array{Float64, N}\n",
    "    gradient::Array{Float64, N}\n",
    "    v₁::Array{Float64, N}\n",
    "    v₂::Array{Float64, N}\n",
    "    v̂₁::Array{Float64, N}\n",
    "    v̂₂::Array{Float64, N}\n",
    "    Variable(N, output; name = \"?\") = new{N}(name, output, zeros(size(output)), zeros(size(output)), zeros(size(output)), zeros(size(output)), zeros(size(output)))\n",
    "end\n",
    "\n",
    "mutable struct NodeOperator{F, N} <: Operator\n",
    "    name::String\n",
    "    inputs::Vector{Node}\n",
    "    output::Array{Float64, N}\n",
    "    gradient::Array{Float64, N}\n",
    "    NodeOperator(fun, inputs...; name = \"?\", shape=(1,1,1)) = new{typeof(fun), length(shape)}(name, [inputs...], zeros(shape), zeros(shape))\n",
    "end\n",
    "\n",
    "mutable struct RNNOperator{F, N} <: Operator\n",
    "    name::String\n",
    "    h::Array{Float64,2}\n",
    "    inputs::Vector{Node}\n",
    "    output::Array{Float64, N}\n",
    "    gradient::Array{Float64, N}\n",
    "    RNNOperator(fun, h, inputs...; name = \"?\", shape=(1,1,1)) = new{typeof(fun), length(shape)}(name,h, [inputs...], zeros(shape), zeros(shape))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da09b989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_graph (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "function visit(node::Node, visited::Set, order::Vector)\n",
    "    if node ∉ visited\n",
    "        push!(visited, node)\n",
    "        push!(order, node)\n",
    "    end\n",
    "end\n",
    "\n",
    "function visit(node::Operator, visited::Set, order::Vector)\n",
    "    if node ∉ visited\n",
    "        for input in node.inputs\n",
    "            visit(input, visited, order)\n",
    "        end\n",
    "        push!(visited, node)\n",
    "        push!(order, node)\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function create_graph(root::Node)\n",
    "    visited = Set{Node}()\n",
    "    order = Vector{Node}()\n",
    "    visit(root, visited, order)\n",
    "    return order\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4ebd648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show (generic function with 611 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Base: show, summary\n",
    "show(io::IO, x::NodeOperator{F}) where {F} = print(io, \"op \", \"(\", F, \")\");\n",
    "show(io::IO, x::RNNOperator{F}) where {F} = print(io, \"op \", \"(\", F, \")\");\n",
    "show(io::IO, x::Variable) = begin\n",
    "    print(io, \"var \", x.name);\n",
    "    print(io, \"\\n ┣━ ^ \"); summary(io, x.output)\n",
    "    print(io, \"\\n ┗━ ∇ \");  summary(io, x.gradient)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4db448f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "zero_gradient!(node::Node) = fill!(node.gradient, 0)\n",
    "\n",
    "compute!(node::Variable) = nothing\n",
    "compute!(node::Operator) = node.output .= forward(node, [input.output for input in node.inputs]...)\n",
    "\n",
    "function forward!(order::Vector{Node})::Float64\n",
    "    for node in order\n",
    "        compute!(node)\n",
    "        zero_gradient!(node)\n",
    "    end\n",
    "    \n",
    "    return last(order).output[1]\n",
    "end   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a28d4e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 3 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "update!(node::Node, gradient) = if isempty(node.gradient)\n",
    "    node.gradient = gradient else node.gradient .+= gradient\n",
    "end\n",
    "\n",
    "function backward!(order::Vector; seed = 1.0)\n",
    "    result = last(order)\n",
    "    result.gradient .= [seed]\n",
    "    \n",
    "    for node in reverse(order)\n",
    "        backward!(node)\n",
    "    end\n",
    "end\n",
    "\n",
    "backward!(node::Variable) = nothing\n",
    "\n",
    "function backward!(node::Operator)\n",
    "    backward(node, [input.output for input in node.inputs]..., node.gradient)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98249c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct RNNParams\n",
    "    W::Variable{2}\n",
    "    U::Variable{2}\n",
    "    b::Variable{1}\n",
    "end\n",
    "\n",
    "struct DenseParams\n",
    "    weights::Variable{2}\n",
    "    bias::Variable{1}\n",
    "end\n",
    "struct NetworkParams\n",
    "    rnn::RNNParams\n",
    "    dense::DenseParams\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38707839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recurent_layer(x::Node, h, W::Node,U::Node, b::Node, shape) = RNNOperator(recurent_layer,h,name=\"rnn\",shape=shape,x,W,U,b)\n",
    "@timeit to \"recurent forward\" forward(node::RNNOperator{typeof(recurent_layer)}, x, W, U, b) = let\n",
    "   fill!(node.h,0)\n",
    "    @views x_cur =  x[1:recsize]\n",
    "    node.h[:,1]  = tanh.(W*node.h[:,1]+U*x_cur+b)   \n",
    "    for t in range(2,layerNumber)\n",
    "       @views x_cur =  x[(t-1)*(recsize)+1:(t)*(recsize)]\n",
    "       node.h[:,t] = tanh.(W*node.h[:,t-1]+U*x_cur+b)   \n",
    "    end\n",
    "    return node.h[:,layerNumber]\n",
    "end\n",
    "\n",
    "@timeit to \"recurent backward\" backward(node::RNNOperator{typeof(recurent_layer)}, x, W, U, b, g) = let\n",
    "   gradienth = zeros(size(node.gradient)[1],layerNumber)\n",
    "   gradienth[:,layerNumber] = node.gradient\n",
    "  gradientW =  node.inputs[2].gradient\n",
    "  gradientU = node.inputs[3].gradient\n",
    "  gradientB = node.inputs[4].gradient\n",
    " t = layerNumber-1\n",
    "  while t >=1\n",
    "    gradienth[:,t] = ((node.inputs[2].output'*gradienth[:,t+1])'*diagm(1 .- node.h[:,t+1].^2) )'\n",
    "    diag = diagm(1 .- node.h[:,t].^2);\n",
    "    t-=1\n",
    " end\n",
    " t = layerNumber\n",
    " while t>=1\n",
    "   diag = diagm(1 .- node.h[:,t].^2);\n",
    "    gradientU += diag*gradienth[:,t]*node.inputs[1].output[(t-1)*(recsize)+1:(t)*(recsize)]'\n",
    "    gradientB += diag*gradienth[:,t]\n",
    "    t-=1\n",
    " end\n",
    " t = layerNumber\n",
    " while t>1\n",
    "   diag = diagm(1 .- node.h[:,t].^2);\n",
    "   gradientW += diag*gradienth[:,t]*node.h[:,t-1]'\n",
    "   t-=1\n",
    " end\n",
    "      node.inputs[2].gradient = gradientW \n",
    "      node.inputs[3].gradient = gradientU\n",
    "      node.inputs[4].gradient = gradientB\n",
    "    return tuple(zeros(size(x)), gradientW, gradientU, gradientB)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba0a1cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dense_layer(x::Node, w::Node, b::Node, shape) = NodeOperator(dense_layer, name=\"dense\", shape=shape, x, w, b)\n",
    "\n",
    "@timeit to \"dense forward\" forward(::NodeOperator{typeof(dense_layer)}, x, w, b) = let\n",
    "    return w * x + b\n",
    "end\n",
    "\n",
    "@timeit to \"dense backward\" backward(node::NodeOperator{typeof(dense_layer)}, x, w, b, g) = let\n",
    "    node.inputs[1].gradient = w' * g\n",
    "    node.inputs[2].gradient = g * x'\n",
    "    node.inputs[3].gradient = g\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3afd94db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "softmax(x::Node, shape) = NodeOperator(softmax, name=\"softmax\", shape=shape, x)\n",
    "\n",
    "@timeit to \"softmax forward\" forward(::NodeOperator{typeof(softmax)}, x) = let\n",
    "    return exp.(x) ./ sum(exp.(x))\n",
    "end\n",
    "\n",
    "@timeit to \"softmax backward\" backward(node::NodeOperator{typeof(softmax)}, x, g) = let\n",
    "    y = node.output\n",
    "    J = diagm(y) .- y * y'\n",
    "    node.inputs[1].gradient = (J' * g)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48f85779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross_entropy_loss(ŷ::Node, y::Node, shape) = NodeOperator(cross_entropy_loss, name=\"cross_entropy_loss\", shape=shape,  ŷ, y)\n",
    "\n",
    "@timeit to \"cross_entropy_loss forward\" forward(::NodeOperator{typeof(cross_entropy_loss)}, ŷ, y) = let\n",
    "    return sum((ŷ-y) .^ 2 ./ 10)\n",
    "end\n",
    "\n",
    "@timeit to \"cross_entropy_loss backward\" backward(node::NodeOperator{typeof(cross_entropy_loss)}, ŷ, y, g) = let\n",
    "    x = zeros(10)\n",
    "    node.inputs[1].gradient = (ŷ-y)/5\n",
    "    node.inputs[2].gradient =[0.0]\n",
    "end\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "551e0a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_network (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function create_network(x::Variable{1}, y::Variable{1}, params::NetworkParams)\n",
    "    x₁ = recurent_layer(x,zeros(64,layerNumber),params.rnn.W,params.rnn.U,params.rnn.b,(64))\n",
    "    x₂ = dense_layer(x₁,params.dense.weights,params.dense.bias,(10))\n",
    "    ŷ = softmax(x₂, (10))\n",
    "    loss = cross_entropy_loss(ŷ, y, (1))\n",
    "    return create_graph(loss)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "41d18687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "he_weights_init (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function he_weights_init(prev, shape...)\n",
    "    std = sqrt(2.0/prev)\n",
    "    weights = rand(Float64, shape) .*2 .-1\n",
    "    return weights .* std\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7565f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mutable struct Adam\n",
    "    α::Float64\n",
    "    ε::Float64\n",
    "    m₁::Float64\n",
    "    m₂::Float64\n",
    "    k::Int64\n",
    "    Adam(α=0.001, m₁=0.9, m₂=0.999, ε=1e-8) = new(α, ε, m₁, m₂, 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a6961444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_weights_N! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function update_weights!(graph, M::Adam)\n",
    "    for node in graph\n",
    "        if (typeof(node) == Variable{1} || typeof(node) == Variable{2}) && (node.name !=\"x\"&&node.name!=\"y\")\n",
    "            update_weights_N!(node, M)\n",
    "        end\n",
    "    end\n",
    "    M.k += 1\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function update_weights_N!(node::Variable{T}, M::Adam) where T\n",
    "    g = node.gradient\n",
    "    v₁ = node.v₁\n",
    "    v₂ = node.v₂\n",
    "    v̂₁ = node.v̂₁\n",
    "    v̂₂ = node.v̂₂\n",
    "    m₁, m₂, k, α, ε = M.m₁, M.m₂, M.k, M.α, M.ε\n",
    "    v₁ .= @. m₁ * v₁ + (1.0 - m₁) * g\n",
    "    v₂ .= @. m₂ * v₂ + (1.0 - m₂) * (g .* g)\n",
    "\n",
    "    v̂₁ .= v₁ ./ (1.0 - m₁^k)\n",
    "    v̂₂ .= v₂ ./ (1.0 - m₂^k)\n",
    "\n",
    "    node.output .-= @. α*v̂₁ / (sqrt(v̂₂) + ε)\n",
    "    \n",
    "    nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec8ec226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "validate (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function validate(x, y, graph, test_data)::Float64\n",
    "    correct = 0\n",
    "    correct_class = zeros(10)\n",
    "    (x_data,y_data) = loader(test_data)\n",
    "    length = size(y_data)[2]\n",
    "    for i in range(1,length)\n",
    "        x.output = x_data[:,i]\n",
    "        y.output = y_data[:,i]\n",
    "        forward!(graph)\n",
    "        pred = argmax(graph[9].output)\n",
    "        if 1 == y_data[:,i][pred]\n",
    "            correct += 1\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    acc_val = correct/size(y_data)[2]\n",
    "    \n",
    "    return acc_val\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ade2daf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetworkParams(RNNParams(var W1\n",
       " ┣━ ^ 64×64 Matrix{Float64}\n",
       " ┗━ ∇ 64×64 Matrix{Float64}, var U1\n",
       " ┣━ ^ 64×196 Matrix{Float64}\n",
       " ┗━ ∇ 64×196 Matrix{Float64}, var b1\n",
       " ┣━ ^ 64-element Vector{Float64}\n",
       " ┗━ ∇ 64-element Vector{Float64}), DenseParams(var w2\n",
       " ┣━ ^ 10×64 Matrix{Float64}\n",
       " ┗━ ∇ 10×64 Matrix{Float64}, var b2\n",
       " ┣━ ^ 10-element Vector{Float64}\n",
       " ┗━ ∇ 10-element Vector{Float64}))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recsize = 14*14\n",
    "\n",
    "recurent = RNNParams(\n",
    "    Variable(2, he_weights_init(64, 64, 64), name=\"W1\"),\n",
    "    Variable(2, he_weights_init(64, 64, recsize), name=\"U1\"),\n",
    "    Variable(1, zeros(64), name=\"b1\")\n",
    ")\n",
    "dense = DenseParams(\n",
    "    Variable(2, he_weights_init(64, 10, 64), name=\"w2\"),\n",
    "    Variable(1, zeros(10), name=\"b2\")\n",
    ")\n",
    "networkparams = NetworkParams(recurent,dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0798c4f4-b616-4f8d-aebb-eeb66b89507c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11-element Vector{Node}:\n",
       " var x\n",
       " ┣━ ^ 784-element Vector{Float64}\n",
       " ┗━ ∇ 784-element Vector{Float64}\n",
       " var W1\n",
       " ┣━ ^ 64×64 Matrix{Float64}\n",
       " ┗━ ∇ 64×64 Matrix{Float64}\n",
       " var U1\n",
       " ┣━ ^ 64×196 Matrix{Float64}\n",
       " ┗━ ∇ 64×196 Matrix{Float64}\n",
       " var b1\n",
       " ┣━ ^ 64-element Vector{Float64}\n",
       " ┗━ ∇ 64-element Vector{Float64}\n",
       " op (typeof(recurent_layer))\n",
       " var w2\n",
       " ┣━ ^ 10×64 Matrix{Float64}\n",
       " ┗━ ∇ 10×64 Matrix{Float64}\n",
       " var b2\n",
       " ┣━ ^ 10-element Vector{Float64}\n",
       " ┗━ ∇ 10-element Vector{Float64}\n",
       " op (typeof(dense_layer))\n",
       " op (typeof(softmax))\n",
       " var y\n",
       " ┣━ ^ 10-element Vector{Float64}\n",
       " ┗━ ∇ 10-element Vector{Float64}\n",
       " op (typeof(cross_entropy_loss))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Opracowane na podstawie https://minpy.readthedocs.io/en/latest/tutorial/rnn_mnist.html\n",
    "using MLDatasets, Flux\n",
    "train_data = MLDatasets.MNIST(split=:train)\n",
    "test_data  = MLDatasets.MNIST(split=:test)\n",
    "\n",
    "function loader(data)\n",
    "    x1dim = reshape(data.features, 28 * 28, :) # reshape 28×28 pixels into a vector of pixels\n",
    "    yhot  = Flux.onehotbatch(data.targets, 0:9) # make a 10×60000 OneHotMatrix\n",
    "    (x1dim, yhot)\n",
    "end\n",
    "(x_data,y_data) = loader(train_data)\n",
    "x::Variable{1} = Variable(1, x_data[:,1], name=\"x\")\n",
    "y::Variable{1} = Variable(1, y_data[:,1], name=\"y\")\n",
    "layerNumber = floor(Int,size(x_data[:,1])[1]/recsize)\n",
    "net = create_network(x,y,networkparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4372550b-a7e1-4f2b-88b0-7dac8e5669be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46.780334 seconds (37.79 M allocations: 84.525 GiB, 9.71% gc time, 2.98% compilation time)\n",
      "Epoch: 1\tAverage loss: 0.014\tAverage acc: 0.904\tAverage test acc: 0.906\n",
      " 42.950336 seconds (37.07 M allocations: 84.478 GiB, 8.79% gc time)\n",
      "Epoch: 2\tAverage loss: 0.012\tAverage acc: 0.922\tAverage test acc: 0.912\n",
      " 43.120719 seconds (37.07 M allocations: 84.478 GiB, 9.07% gc time)\n",
      "Epoch: 3\tAverage loss: 0.012\tAverage acc: 0.92\tAverage test acc: 0.916\n",
      " 42.568344 seconds (37.07 M allocations: 84.478 GiB, 8.91% gc time)\n",
      "Epoch: 4\tAverage loss: 0.012\tAverage acc: 0.923\tAverage test acc: 0.915\n",
      " 41.379916 seconds (37.07 M allocations: 84.478 GiB, 8.83% gc time)\n",
      "Epoch: 5\tAverage loss: 0.012\tAverage acc: 0.923\tAverage test acc: 0.921\n",
      "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────────────────\u001b[22m\n",
      "\u001b[0m\u001b[1m                               \u001b[22m         Time                    Allocations      \n",
      "                               ───────────────────────   ────────────────────────\n",
      "       Tot / % measured:             221s /  97.5%            424GiB /  99.8%    \n",
      "\n",
      " Section               ncalls     time    %tot     avg     alloc    %tot      avg\n",
      " ────────────────────────────────────────────────────────────────────────────────\n",
      " backward                300k     162s   75.0%   540μs    418GiB   98.8%  1.43MiB\n",
      "   recurent backward     300k     156s   72.3%   520μs    415GiB   98.1%  1.42MiB\n",
      "   dense backward        300k    772ms    0.4%  2.57μs   1.64GiB    0.4%  5.72KiB\n",
      "   softmax backward      300k    541ms    0.3%  1.80μs    819MiB    0.2%  2.80KiB\n",
      "   cross_entropy_lo...   300k    333ms    0.2%  1.11μs    151MiB    0.0%     528B\n",
      " update weights          300k    37.1s   17.2%   124μs   39.8MiB    0.0%     139B\n",
      " forward                 300k    15.6s    7.2%  52.0μs   4.38GiB    1.0%  15.3KiB\n",
      "   recurent forward      300k    7.57s    3.5%  25.2μs   3.85GiB    0.9%  13.5KiB\n",
      "   dense forward         300k    283ms    0.1%   945ns   91.6MiB    0.0%     320B\n",
      "   softmax forward       300k    152ms    0.1%   506ns   91.6MiB    0.0%     320B\n",
      "   cross_entropy_lo...   300k    147ms    0.1%   490ns   96.1MiB    0.0%     336B\n",
      " recurent forward       50.0k    1.15s    0.5%  22.9μs    657MiB    0.2%  13.5KiB\n",
      " dense forward          50.0k   33.8ms    0.0%   677ns   15.3MiB    0.0%     320B\n",
      " cross_entropy_loss...  50.0k   31.1ms    0.0%   622ns   16.0MiB    0.0%     336B\n",
      " softmax forward        50.0k   20.3ms    0.0%   406ns   15.3MiB    0.0%     320B\n",
      "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────────────────\u001b[22m"
     ]
    }
   ],
   "source": [
    "using ProgressMeter\n",
    "loss::Float64 = 0.0\n",
    "pred::UInt8 = 0\n",
    "epochs = 5\n",
    "losses = zeros(epochs)\n",
    "acc = zeros(epochs)\n",
    "test_acc = zeros(epochs)\n",
    "correct = 0 \n",
    "adam = Adam()\n",
    "for epoch in 1:epochs\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    length = size(y_data)[2]\n",
    "    @time for i in range(1,length)\n",
    "         x.output .= @view x_data[:,i]\n",
    "         y.output .= @view y_data[:,i]\n",
    "        @timeit to \"forward\" loss += forward!(net)\n",
    "        prob::NodeOperator{typeof(softmax), 1} = net[9]\n",
    "        pred = argmax(prob.output)\n",
    "        if 1 == y_data[:,i][pred]\n",
    "            correct += 1\n",
    "        end\n",
    "        @timeit to \"backward\" backward!(net)\n",
    "        \n",
    "        @timeit to \"update weights\" update_weights!(net, adam)\n",
    "    end\n",
    "    losses[epoch] = loss/length\n",
    "    acc[epoch] = correct/length\n",
    "    test_acc[epoch] = validate(x,y,net,test_data)\n",
    "    println(\"Epoch: \", epoch, \"\\tAverage loss: \", round(losses[epoch], digits=3), \"\\tAverage acc: \", round(acc[epoch],digits=3),\"\\tAverage test acc: \",round(test_acc[epoch],digits=3))\n",
    "end\n",
    "show(to)\n",
    "reset_timer!(to);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
